{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00109ac8-1e92-47ff-a69b-4ac97de25516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from useful_functions import data_compiler\n",
    "from useful_functions import month_genesis_locs\n",
    "from useful_functions import grid_counter\n",
    "from useful_functions import environmental_select_stack\n",
    "from useful_functions import create_genesis_grid_labels\n",
    "from useful_functions import take_closest_point\n",
    "from useful_functions import temporal_spatial_smoothing\n",
    "from distance import distance_calculator\n",
    "from distance import pointdist_calc\n",
    "import os\n",
    "import tropycal.tracks as tracks\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import cartopy.crs as ccrs\n",
    "import copy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import regionmask\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90887c75-024e-457d-9cbd-6f8386ea3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "gdf = gpd.read_file(\"/glade/work/acheung/Initial_RF_Datasets/shape_files_regions/World_Seas_IHO_v3.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894d420a-4d9c-4f54-a73b-155ab2041fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latitude=np.arange(0,60,2)\n",
    "Longitude=np.arange(260,360,2)\n",
    "# Create masks based on regions\n",
    "NAO_polygon = gdf.loc[(gdf['NAME'] == 'North Atlantic Ocean') | (gdf['NAME'] == 'Gulf of Mexico') | (gdf['NAME'] == 'Caribbean Sea')]\n",
    "lon_adj = np.where(Longitude > 180, Longitude - 360,Longitude)\n",
    "NAOmask = regionmask.mask_geopandas(NAO_polygon, Longitude, Latitude,lon_name='Longitude', lat_name='Latitude',wrap_lon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a8165b-8ee7-4dc1-80ef-e671f6e99636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile Environmental Data\n",
    "vars_list = os.listdir(\"/glade/work/acheung/TC_Genesis\") # Determine Variables\n",
    "vars_list.remove('.DS_Store')\n",
    "vars_list.remove('CRH_ERA5')\n",
    "all_vars_list = []\n",
    "for vars_list_now in vars_list:\n",
    "    Varcompiled, YearList,lons,lats,arr = data_compiler(vars_list_now)\n",
    "    all_vars_list.append(Varcompiled)\n",
    "\n",
    "arr = xr.DataArray(all_vars_list,coords=[(\"Variable\",vars_list),(\"Year\",range(1950,2021)),(\"Latitude\",lats),(\"Longitude\",lons),(\"Month\",range(1,13))])\n",
    "\n",
    "# # Time ranges for EVERYTHING desired \n",
    "# NOTE: Keep in mind that when switching these values... \n",
    "# you may need to adjust create_genesis_grid_labels in useful_functions and also rerun/uncomment the code below!\n",
    "\n",
    "# Set time parameters\n",
    "month_range = np.arange(1,13,1)\n",
    "test_years = np.arange(2005,2021,1)\n",
    "train_years = np.arange(1950,2005,1)\n",
    "train_years = np.setdiff1d(train_years, np.arange(1971,1979,1))\n",
    "\n",
    "# Grab environmental data from functions for the set time parameters\n",
    "train_stacked,train_NaNlocs = environmental_select_stack(month_range,train_years,Latitude,Longitude,arr)\n",
    "# Apply land/ocean mask to environmental training information\n",
    "train_stacked = (train_stacked.unstack()).where(NAOmask.isnull() == False).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "\n",
    "test_stacked,test_NaNlocs = environmental_select_stack(month_range,test_years,Latitude,Longitude,arr)\n",
    "\n",
    "# Apply land/ocean mask to environmental training information\n",
    "test_stacked = (test_stacked.unstack()).where(NAOmask.isnull() == False).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\")) # This is a mask to restrict to only ocean\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Normalize environmental data for training\n",
    "\n",
    "# Min/Max Scaling (Normalization)\n",
    "train_stacked_copy = copy.deepcopy(train_stacked) # For modifying (normalized)\n",
    "train_stacked_copy_2 = copy.deepcopy(train_stacked) # For referencing \n",
    "train_stacked_copy_3 = copy.deepcopy(train_stacked) # For storing anomaly (not normalized)\n",
    "test_stacked_copy = copy.deepcopy(test_stacked)\n",
    "test_stacked_copy_2 = copy.deepcopy(test_stacked)\n",
    "test_stacked_copy_3 = copy.deepcopy(test_stacked)\n",
    "\n",
    "# ------------------------------------------Important setting-------------------------------------------------------\n",
    "anomaly = 0 # 1 = Set for anomaly\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for varloops in range(train_stacked_copy.shape[0]): \n",
    "    ## Find anomalies and then normalize training Data or not.\n",
    "    \n",
    "    if anomaly == 1: # If anomaly desired, this is equal to 1; Base anomalies off training monthly means\n",
    "    \n",
    "    # Find Training Data Anomalies\n",
    "    \n",
    "        train_monthly_avg = train_stacked_copy_2[varloops].unstack().mean([\"Year\"]).stack(z=(\"Month\",\"Latitude\",\"Longitude\"))\n",
    "        # Calculate Anomaly (subtract values from monthly averages)\n",
    "        train_anomaly = (train_stacked_copy_2[varloops].unstack()-train_monthly_avg.unstack()).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "        # Save anomalies before normalization\n",
    "        train_stacked_copy_3[varloops,:] = train_anomaly\n",
    "        # Normalize the anomalies and save\n",
    "        train_stacked_copy[varloops,:] = (train_anomaly - train_anomaly.min())/(train_anomaly.max()-train_anomaly.min())\n",
    "        \n",
    "    # Find Testing Data Anomalies (NOTE: Based off training monthly means)\n",
    "    \n",
    "        # test_monthly_avg = test_stacked_copy_2[varloops].unstack().mean([\"Year\"]).stack(z=(\"Month\",\"Latitude\",\"Longitude\"))\n",
    "        # Calculate Anomaly (subtract values from monthly averages)\n",
    "        test_anomaly = (test_stacked_copy_2[varloops].unstack()-train_monthly_avg.unstack()).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "        # Save anomalies before normalization\n",
    "        test_stacked_copy_3[varloops,:] = test_anomaly\n",
    "        # Normalize the anomalies (KEEP IN MIND testing min/max normalization values are based off training values)\n",
    "        test_stacked_copy[varloops,:] = (test_anomaly - train_anomaly.min())/(train_anomaly.max()-train_anomaly.min())           \n",
    "    \n",
    "    else: # If not predicting anomalies, simply normalization\n",
    "        train_stacked_copy[varloops,:] = (train_stacked_copy_2[varloops] - train_stacked_copy_2[varloops].min())/(train_stacked_copy_2[varloops].max()-train_stacked_copy_2[varloops].min())\n",
    "        # Normalize Testing Data Using Training Data Range and Minimum (KEEP IN MIND testing min/max normalization values use training values)\n",
    "        test_stacked_copy[varloops,:] = (test_stacked_copy_2[varloops] - train_stacked_copy_2[varloops].min())/(train_stacked_copy_2[varloops].max()-train_stacked_copy_2[varloops].min())\n",
    "\n",
    "# Apply masks and missing data locations\n",
    "train_env_data = train_stacked_copy.drop_isel(z=train_NaNlocs).transpose() # This is locations where there is no data\n",
    "\n",
    "test_env_data = test_stacked_copy.drop_isel(z=test_NaNlocs).transpose()\n",
    "\n",
    "## This is if we want anomaly, but with no normalization.\n",
    "# if anomaly == 1:\n",
    "#     train_env_data_no_norm = train_stacked_copy_3.drop_isel(z=train_NaNlocs).transpose()\n",
    "#     test_env_data_no_norm = test_stacked_copy_3.drop_isel(z=test_NaNlocs).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3a4d0a-b1b3-4294-b3dd-550b45c2c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab ibtracs data (uncomment this section if you want to recreate genesis (observed) labels, otherwise it is already saved)\n",
    "# basin_dataset = tracks.TrackDataset(basin='north_atlantic',source='ibtracs',include_btk=True)\n",
    "\n",
    "# # Create genesis labels from ibtracs data (slowest step)\n",
    "# vmin=0\n",
    "# print('Training Dataset')\n",
    "# train_labels_predropped = create_genesis_grid_labels(month_range,train_years,vmin,basin_dataset)\n",
    "# print('Testing Dataset')\n",
    "# test_labels_predropped = create_genesis_grid_labels(month_range,test_years,vmin,basin_dataset)\n",
    "\n",
    "# train_labels_predropped.unstack().to_dataset(name='Genesis_Grids').to_netcdf(\"/glade/work/acheung/Initial_RF_Datasets/train_labels_predropped.nc\")\n",
    "# test_labels_predropped.unstack().to_dataset(name='Genesis_Grids').to_netcdf(\"/glade/work/acheung/Initial_RF_Datasets/test_labels_predropped.nc\")\n",
    "# # --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f351375-f43a-4a00-9e1f-d0bb24f48c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in already created genesis (obs) labels\n",
    "train_labels_predropped_read = xr.open_dataset(\"/glade/work/acheung/Initial_RF_Datasets/train_labels_predropped.nc\")['Genesis_Grids']\n",
    "test_labels_predropped_read = xr.open_dataset(\"/glade/work/acheung/Initial_RF_Datasets/test_labels_predropped.nc\")['Genesis_Grids']\n",
    "\n",
    "# Smooth Training Labels\n",
    "smoothed_training_labels = temporal_spatial_smoothing(train_labels_predropped_read.astype(float),0.5,1)\n",
    "smoothed_training_labels_dropped = smoothed_training_labels.stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\")).drop_isel(z=train_NaNlocs).transpose()\n",
    "\n",
    "# Apply land/ocean mask to training labels\n",
    "smoothed_training_labels_dropped = smoothed_training_labels_dropped.unstack().where(NAOmask.isnull() == False).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "\n",
    "# Make training anomaly labels (base climatology off training dataset)\n",
    "if anomaly == 1:\n",
    "    trainingmonthlymeans = smoothed_training_labels_dropped.unstack().mean('Year')\n",
    "    smoothed_training_labels_dropped = (smoothed_training_labels_dropped.unstack() - trainingmonthlymeans).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "    \n",
    "# Smooth Testing Labels (For verification purposes)\n",
    "smoothed_testing_labels = temporal_spatial_smoothing(test_labels_predropped_read.astype(float),0.5,1)\n",
    "smoothed_testing_labels_dropped = smoothed_testing_labels.stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\")).drop_isel(z=test_NaNlocs).transpose()\n",
    "\n",
    "# Apply land/ocean mask to training labels\n",
    "smoothed_testing_labels_dropped = smoothed_testing_labels_dropped.unstack().where(NAOmask.isnull() == False).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "\n",
    "# Make testing anomaly labels (base climatology off training dataset)\n",
    "if anomaly == 1: # Make anomaly labels (base climatology off training dataset)\n",
    "    smoothed_testing_labels_dropped = (smoothed_testing_labels_dropped.unstack() - trainingmonthlymeans).stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\"))\n",
    "    \n",
    "# Note that above cuts off smoothed labeled values over land, thus changing the sum (may need to revisit)\n",
    "\n",
    "# # Stack Test Labels\n",
    "# test_labels_stacked = test_labels_predropped_read.stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\")).drop_isel(z=test_NaNlocs).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0b7ca-de54-45e6-8a6f-a13cf138e369",
   "metadata": {},
   "source": [
    "# !!! Uncomment Fit Model Cell if want to fit model !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08cdea-0477-4e10-8442-e97e69ba5c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit RF Model\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_env_data.dropna(dim='z'), smoothed_training_labels_dropped.dropna(dim='z'))\n",
    "# Predict\n",
    "test_ML_Predict = clf.predict(test_env_data.dropna(dim='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39cd39-29bd-4198-9707-d32ae856799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to keep track of the places that NaNs were dropped out places that have real data due to the masking\n",
    "\n",
    "# Recover dropped NaN Labels (labels)\n",
    "Testing_mask_nan_locs = np.where(smoothed_testing_labels_dropped.isnull() == True)[0] # All nan locations\n",
    "Testing_mask_data_locs = np.where(smoothed_testing_labels_dropped.isnull() == False)[0] # All data locations\n",
    "\n",
    "testing_PD = pd.DataFrame(test_ML_Predict,index=Testing_mask_data_locs,columns=['Prediction'])\n",
    "testing_nan_PD = pd.DataFrame(np.nan, index=Testing_mask_nan_locs,columns=['Prediction'])\n",
    "testing_reattachedpd = pd.concat([testing_PD,testing_nan_PD]).sort_index()\n",
    "\n",
    "# Put reNan'd ML Testing Results into Xarray\n",
    "testing_reattached_xr = xr.DataArray(testing_reattachedpd['Prediction'],coords = smoothed_testing_labels_dropped.coords).unstack()\n",
    "\n",
    "# Save ML results to netCDF\n",
    "if anomaly == 1:\n",
    "    testing_reattached_xr.to_dataset(name='Testing_Results').to_netcdf(\"/glade/work/acheung/Initial_RF_Datasets/ml_test_results_with_anomaly.nc\")\n",
    "else:\n",
    "    testing_reattached_xr.to_dataset(name='Testing_Results').to_netcdf(\"/glade/work/acheung/Initial_RF_Datasets/ml_test_results_without_anomaly.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6a05d-2581-4b7e-b8d1-5f675a92ab5b",
   "metadata": {},
   "source": [
    "# Below this cell involves viewing/quantifying the results of the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2835b-a671-4e75-a8ba-bab64a1dc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Saved Dataset\n",
    "# Change file path to view different saved ML results!\n",
    "if anomaly == 1:\n",
    "    test_ML_Predict_XR = xr.open_dataset(\"/glade/work/acheung/Initial_RF_Datasets/ml_test_results_with_anomaly.nc\")['Testing_Results']\n",
    "else:\n",
    "    test_ML_Predict_XR = xr.open_dataset(\"/glade/work/acheung/Initial_RF_Datasets/ml_test_results_without_anomaly.nc\")['Testing_Results']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ff2f2-9acb-4d9c-a602-7f69808e3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_reattached_xr.unstack().sel(Month=8,Year=2005).plot(vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d59138-fa17-4272-aef5-b7a5e1bf256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Verification Scores\n",
    "\n",
    "# 1. RMSE \n",
    "y_true = smoothed_testing_labels_dropped.dropna(dim='z')\n",
    "y_pred = test_ML_Predict_XR.stack(z=(\"Month\",\"Latitude\",\"Longitude\",\"Year\")).dropna(dim='z')\n",
    "RMSEval = mean_squared_error(y_true, y_pred,squared = False)\n",
    "\n",
    "#2. Pearson Correlation\n",
    "pearson_corr = stats.pearsonr(y_true, y_pred)[0] # Default: Two-sided\n",
    "pearson_p_val = stats.pearsonr(y_true, y_pred)[1]\n",
    "\n",
    "# Print values\n",
    "anomalystr = 'With Anomaly Predictors'\n",
    "withoutanomalystr = 'Without Anomaly Predictors'\n",
    "if anomaly == 1:\n",
    "    score_str = 'RMSE: ' + str(RMSEval) + ', Pearson Corr: ' + str(pearson_corr) + ', P-Value: ' + str(pearson_p_val) + ', ' + anomalystr\n",
    "    print(score_str)\n",
    "else:\n",
    "    score_str = 'RMSE: ' + str(RMSEval) + ', Pearson Corr: ' + str(pearson_corr) + ', P-Value: ' + str(pearson_p_val) + ', ' + withoutanomalystr\n",
    "    print(score_str)\n",
    "\n",
    "# Write String to text file\n",
    "\n",
    "#open text file\n",
    "if anomaly == 1:\n",
    "    text_file = open(\"/glade/work/acheung/Initial_RF_Datasets/anomaly_scores.txt\", \"w\")\n",
    "else:\n",
    "    text_file = open(\"/glade/work/acheung/Initial_RF_Datasets/no_anomaly_scores.txt\", \"w\")\n",
    "\n",
    "text_file.write(score_str)\n",
    "#close file\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be283e09-7ad8-481c-9da8-8f110b47b09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot data\n",
    "for test_years_now in test_years:\n",
    "    mo_view = 8\n",
    "    yr_view = test_years_now\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_global()\n",
    "    ax.coastlines()\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    ax.set_extent([-100, 0, 0, 50])\n",
    "    (test_ML_Predict_XR.unstack().sel(Month=mo_view,Year=yr_view)).plot(vmin=0,vmax=0.03)\n",
    "\n",
    "    scat = test_labels_predropped_read.sel(Month=mo_view,Year=yr_view).stack(new=['Latitude','Longitude'])\n",
    "    scat_dropped = scat.where(scat>0,drop=True)\n",
    "    ax.scatter(scat_dropped['Longitude'],scat_dropped['Latitude'],marker='x',color='r')\n",
    "    if mo_view > 9:\n",
    "        mostr = str(mo_view)\n",
    "    else:\n",
    "        mostr = '0'+str(mo_view)\n",
    "    timestr = mostr + '_' + str(yr_view)\n",
    "    \n",
    "    plt.savefig('/glade/work/acheung/RF_V2_Plots/RF_Reg_plot'+'_'+timestr+'.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727263a-21b5-4a17-ba40-e6c07fb22de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Environmental Data\n",
    "plt.figure()\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True)\n",
    "ax.set_extent([-100, 0, 0, 50])\n",
    "test_env_data[:,3].unstack().sel(Month=8,Year=2005).plot()\n",
    "plt.savefig(\"Anomaly_Normalized_Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781a7ab-2ee0-4fbc-8918-5f4cb093d7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-squalls]",
   "language": "python",
   "name": "conda-env-.conda-squalls-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
